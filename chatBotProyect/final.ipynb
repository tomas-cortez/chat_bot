{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ad7c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:08:22.946052Z",
     "start_time": "2022-09-23T19:08:22.751373Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc560682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.616710Z",
     "start_time": "2022-09-23T19:06:02.613093Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizador_carreras = pickle.load(open(\"vectorizer/vectorizador_carreras.pkl\",\"rb\"))\n",
    "vectorizador_w5 = pickle.load(open(\"vectorizer/vectorizador_w5.pkl\",\"rb\"))\n",
    "vectorizador_intents = pickle.load(open(\"vectorizer/vectorizador_intents.pkl\",\"rb\"))\n",
    "vectorizador_sub_intents = pickle.load(open(\"vectorizer/vectorizador_sub_intents.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6f7e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.627902Z",
     "start_time": "2022-09-23T19:06:02.617905Z"
    }
   },
   "outputs": [],
   "source": [
    "modelo_carreras = pickle.load(open(\"models/modelo_carreras.sav\",\"rb\"))\n",
    "modelo_w5 = pickle.load(open(\"models/modelo_w5.sav\",\"rb\"))\n",
    "modelo_intents = pickle.load(open(\"models/modelo_intents.sav\",\"rb\"))\n",
    "modelo_sub_intents = pickle.load(open(\"models/modelo_sub_intents.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc303b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.634950Z",
     "start_time": "2022-09-23T19:06:02.628830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generalidades' 'plan' 'tramites' 'pagos' 'trabajo']\n",
      "['todas' 'preinscripcion' 'cies' 'requisitos' 'equivalencias' 'medios'\n",
      " 'precio']\n",
      "['IA' 'MM' 'DG' 'Pub' 'RRPP' 'GCF' 'HyS' 'Log' 'RRHH' 'TUR' 'Agro' 'CI'\n",
      " 'ADMIN' 'MKT' '3D' 'INF' 'Rob' 'Svirt' 'todas']\n",
      "['todas' 'como' 'quien' 'que' 'cuando' 'donde']\n"
     ]
    }
   ],
   "source": [
    "respuestas = pd.read_csv('data/tbl_respuestas.csv', sep=',', encoding='utf_8')\n",
    "respuestas.drop(\"Modalidad\", axis=1, inplace=True)\n",
    "print(respuestas.Intencion.unique())\n",
    "print(respuestas.SubIntencion.unique())\n",
    "print(respuestas.Carrera.unique())\n",
    "print(respuestas.w5.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a848fd6",
   "metadata": {},
   "source": [
    "### PreProcesar y Corregir Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0017ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.644175Z",
     "start_time": "2022-09-23T19:06:02.635891Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def PreProcesar_carrera(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    fc = open('data/stopwords.txt', 'r', encoding='utf8')\n",
    "    stopwords = fc.read().split('\\n')\n",
    "    fc.close()\n",
    "    \n",
    "    fc1 = open('data/stopwords_carreras.txt', 'r', encoding='utf8')\n",
    "    stopwords1 = fc1.read().split('\\n')\n",
    "    stopwords1=[x.lower() for x in stopwords1]\n",
    "\n",
    "    fc1.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] in stopwords1] for oracion in oraciones]\n",
    "\n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "        #print(oraciones)\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        if oracion == \"\":\n",
    "            oracion = \"not_match\"\n",
    "        Corpus_Procesado.append(oracion)\n",
    "    \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_carrera(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_carrera(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def carrera_final(entrada):\n",
    "    \n",
    "    vari_df_textos_carr = entrada.copy()\n",
    "    carrera_corr = Corregir_Documentos_carrera(vari_df_textos_carr,['oracion'],False,True,True)\n",
    "\n",
    "    carrera_corr.loc[carrera_corr.oracion == \"not_match\"] = 'todas'\n",
    "    for tipo in carrera_corr.oracion:\n",
    "        if tipo == 'todas':\n",
    "            carrera2 = ['todas']\n",
    "            carrera = 1.\n",
    "        else:\n",
    "            array_carreras=vectorizador_carreras.transform([carrera_corr['oracion'][0][:-1]])\n",
    "            carrera = sorted(list(modelo_carreras.predict_proba(array_carreras)[0]))[-1]\n",
    "            carrera2 = modelo_carreras.predict(array_carreras)\n",
    "    if carrera < .5: \n",
    "        carrera2 = ['todas']\n",
    "            \n",
    "    return carrera2, carrera\n",
    "\n",
    "#carrera_final(df_textos)\n",
    "\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db40490c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.653405Z",
     "start_time": "2022-09-23T19:06:02.645275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "def PreProcesar_w5(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('data/stopwords_sin_w5.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    #stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    \n",
    "    w5_posibles = list(respuestas.w5.unique())\n",
    "    \n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] in w5_posibles] for oracion in oraciones]\n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        if oracion == \"\":\n",
    "            oracion = \"not_match\"\n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_w5(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_w5(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    #df_textos.loc[df_textos.index == 0, \"oracion\"] = 'todas'\n",
    "    return df_textos\n",
    "\n",
    "\n",
    "def w5_final(entrada):\n",
    "    \n",
    "    vari_df_textos_w5 = df_textos.copy()\n",
    "    w5_corr = Corregir_Documentos_w5(vari_df_textos_w5,['oracion'],False,True,True)\n",
    "\n",
    "    w5_corr.loc[w5_corr.oracion == \"not_match\"] = 'todas'\n",
    "    for tipo in w5_corr.oracion:\n",
    "        if tipo == 'todas':\n",
    "            w5_2 = ['todas']\n",
    "            w5 = 1.\n",
    "        else:\n",
    "            array_w5 = vectorizador_w5.transform([w5_corr['oracion'][0][:-1]])\n",
    "            w5 = sorted(list(modelo_w5.predict_proba(array_w5)[0]))[-1]\n",
    "            w5_2 = modelo_w5.predict(array_w5)        \n",
    "    if w5 < .5: \n",
    "        w5_2 = ['todas']\n",
    "        \n",
    "    return w5_2, w5\n",
    "\n",
    "#w5_final(df_textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085a7af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:02.664125Z",
     "start_time": "2022-09-23T19:06:02.654695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "def PreProcesar_intents(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('data/stopwords_intents.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        if oracion == \"\":\n",
    "            oracion = \"not_match\"\n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_intents(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_intents(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def inte_final(entrada):\n",
    "    \n",
    "    vari_df_textos_inte = df_textos.copy()\n",
    "    inte_corr = Corregir_Documentos_intents(vari_df_textos_inte,['oracion'],False,True,True)\n",
    "    inte_corr.loc[inte_corr.oracion == \"not_match\"] = 'generalidades'\n",
    "    for tipo in inte_corr.oracion:\n",
    "        if tipo == 'generalidades':\n",
    "            intents_2 = ['generalidades']\n",
    "            intents = 1.\n",
    "        else:\n",
    "            array_intents = vectorizador_intents.transform([inte_corr['oracion'][0][:-1]])\n",
    "            intents = sorted(list(modelo_intents.predict_proba(array_intents)[0]))[-1]\n",
    "            intents_2 = modelo_intents.predict(array_intents)\n",
    "\n",
    "    if intents < .5: \n",
    "        intents_2 = ['generalidades']\n",
    "            \n",
    "    return intents_2, intents\n",
    "\n",
    "#inte_final(df_textos)\n",
    "\n",
    "def sub_final(entrada):\n",
    "    \n",
    "    vari_df_textos_sub = df_textos.copy()\n",
    "    sub_corr = Corregir_Documentos_intents(vari_df_textos_sub,['oracion'],False,True,True)\n",
    "    sub_corr.loc[sub_corr.oracion == \"not_match\"] = 'todas'\n",
    "    for tipo in sub_corr.oracion:\n",
    "        if tipo == 'todas':\n",
    "            sub_intents_2 = ['todas']\n",
    "            sub_intents = 1.\n",
    "        else:\n",
    "            array_sub_intents=vectorizador_sub_intents.transform([sub_corr['oracion'][0][:-1]])\n",
    "            sub_intents = sorted(list(modelo_sub_intents.predict_proba(array_sub_intents)[0]))[-1]\n",
    "            sub_intents_2 = modelo_sub_intents.predict(array_sub_intents)\n",
    "            \n",
    "    if sub_intents < .5: \n",
    "        sub_intents_2 = ['todas']\n",
    "        \n",
    "    return sub_intents_2, sub_intents\n",
    "\n",
    "#sub_final(df_textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1def747",
   "metadata": {},
   "source": [
    "### Listo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18d78c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:07:39.122102Z",
     "start_time": "2022-09-23T19:07:16.726193Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "on = True\n",
    "info = False\n",
    "n = 0\n",
    "cambio = True\n",
    "while on:\n",
    "    \n",
    "    print(colored(\"Ingrese la pregunta a continuacion:\\n\",\"blue\"))\n",
    "    txt=input(colored(\"Tu: \",\"green\"))\n",
    "\n",
    "    df_textos = pd.DataFrame(columns = ['oracion'])\n",
    "    df_textos.loc[0] = [txt]\n",
    "    \n",
    "    listita = [inte_final(df_textos)[0][0], sub_final(df_textos)[0][0],\n",
    "           carrera_final(df_textos)[0][0], w5_final(df_textos)[0][0]]\n",
    "    df_textos_listo = pd.DataFrame(columns = ['Intencion',\"SubIntencion\",\"Carrera\",\"w5\"])\n",
    "    df_textos_listo.loc[0] = listita\n",
    "    \n",
    "    respu = respuestas[((respuestas.Intencion==df_textos_listo[\"Intencion\"].values[0])|(respuestas.Intencion==\"generalidades\"))\\\n",
    "                & ((respuestas.SubIntencion==df_textos_listo[\"SubIntencion\"].values[0])|(respuestas.SubIntencion==\"todas\"))\\\n",
    "                & ((respuestas.Carrera==df_textos_listo[\"Carrera\"].values[0])|(respuestas.Carrera==\"todas\"))\\\n",
    "                & ((respuestas.w5==df_textos_listo[\"w5\"].values[0])|(respuestas.w5==\"todas\"))]\n",
    "    print(\"Respuesta:\")\n",
    "    if len(respu.index) < 1: print(colored(\"No he podido entender la pregunta. ¿Podrias reformularla por favor? \",\"blue\"))                \n",
    "    else: print(str(respu[\"Respuesta\"].values[n]))\n",
    "        \n",
    "    if info:\n",
    "        print(colored(\"\\ndeteccion:\\n\",\"red\"),df_textos_listo)\n",
    "        print(colored(\"\\nclasificacion:\\n\",\"red\"),respu[[\"Intencion\",\"SubIntencion\",\"Carrera\",\"w5\"]],\"\\n\")\n",
    "    \n",
    "    while cambio:\n",
    "        try:\n",
    "            rta=input(colored(\"\\nLa respuesta fue acorde a lo solicitado? s/n \\n\",\"blue\"))\n",
    "            if rta==\"s\":\n",
    "                cambio=False\n",
    "                n = 0\n",
    "            elif rta==\"n\":\n",
    "                n = n+1\n",
    "                print(colored(\"Tal vez esta respuesta sea mas oportuna:\\n\",\"blue\"))\n",
    "                print(str(respu[\"Respuesta\"].values[n]))\n",
    "            elif (rta != \"s\" )| (rta != \"n\"):\n",
    "                print(colored(\"La opcion ingresada es incorrecta \\n\",\"blue\"))\n",
    "        except Exception: \n",
    "            print(\"No he podido encontrar una respuesta acorde. Por favor intente reformular la pregunta.\")\n",
    "            cambio=False\n",
    "    \n",
    "    rta=input(colored(\"Quiere realizar otra pregunta? s/n \\n\",\"blue\"))\n",
    "    if rta==\"s\":\n",
    "        on=True\n",
    "        cambio = True\n",
    "        n = 0\n",
    "    elif rta==\"n\":\n",
    "        on=False\n",
    "    elif (rta != \"s\" )| (rta != \"n\"):\n",
    "        print(colored(\"La opcion ingresada es incorrecta \\n\",\"blue\"))\n",
    "    if on == False:\n",
    "        print(\"\\nAdios!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286e8257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:06:49.479895Z",
     "start_time": "2022-09-23T19:06:49.476122Z"
    }
   },
   "outputs": [],
   "source": [
    "#que requisitos necesito para inscribirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b6d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36cf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "468.844px",
    "left": "741px",
    "right": "20px",
    "top": "84px",
    "width": "563px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
