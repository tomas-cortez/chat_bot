{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "# Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Levantamos la lista de StopWords\n",
    "f = open('stopwords_intents.txt', 'r', encoding='utf8')\n",
    "stopwords = f.read().split('\\n')\n",
    "f.close()\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcesar(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    # Removemos StopWords (finándonos en el lema de cada palabra en vez de su texto!)\n",
    "    # No conviene quitar las StopWords antes de lematizar pues son útiles para ese proceso...\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    # Finalmente: devolver nuevamente una lista de cadenas como la recibida, pero con el contenido\n",
    "    # de cada cadena conformado según los parámetros:\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def Generar_Matriz_BOW(df_textos, columna, binario=False, ngram=(1,2)):\n",
    "    \n",
    "    # Vectorizar, usando CountVectorizer de sklearn.feature_extraction.text\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizador = CountVectorizer(binary=binario, ngram_range=ngram)\n",
    "    X = vectorizador.fit_transform(df_textos[columna])\n",
    "    \n",
    "    # Generar el DataFrame a devolver\n",
    "    df_X = pd.DataFrame(X.toarray(), columns=vectorizador.get_feature_names())\n",
    "    df = df_textos.join(df_X)\n",
    "    \n",
    "    return vectorizador, df\n",
    "\n",
    "def Generar_Matriz_Tfidf(df_textos, columna, ngram=(1,2)):\n",
    "    \n",
    "    # Vectorizar... Directamente usar aquí el TfidfVectorizer de sklearn en vez del CountVectorizer\n",
    "    # (Lleva los mismos parámetros y directamente nos devuelve la matriz con los vectores Tf*Idf)\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizador = TfidfVectorizer(ngram_range=ngram)\n",
    "    X = vectorizador.fit_transform(df_textos[columna])\n",
    "    \n",
    "    # Generar el DataFrame a devolver\n",
    "    df_X = pd.DataFrame(X.toarray(), columns=vectorizador.get_feature_names())\n",
    "    df = df_textos.join(df_X)\n",
    "    \n",
    "    return vectorizador, df\n",
    "\n",
    "def Distancia_Coseno(u, v):\n",
    "    \n",
    "    distancia = 1.0 - (np.dot(u, v) / (np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.square(v)))))\n",
    "    return distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#1. Cargar y corregir el corpus\n",
    "df_textos = pd.read_csv('data_intents.csv', sep=';', encoding='utf_8')\n",
    "df_textos = Corregir_Documentos(df_textos,['oracion'],False,True,True)\n",
    "\n",
    "#2. Modelizar los documentos de df_textos\n",
    "vectorizador, df_textos = Generar_Matriz_Tfidf(df_textos,'oracion',ngram=(1,3))\n",
    "#vectorizador, df_textos = Generar_Matriz_BOW(df_textos,'oracion')\n",
    "\n",
    "#3. Separar el corpus en Train/Test\n",
    "X = df_textos.drop([\"intencion\",\"subIntencion\"],axis=1)\n",
    "y = df_textos[[\"intencion\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=124)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oracion</th>\n",
       "      <th>intenciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qué es el ies?</td>\n",
       "      <td>(charla, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuál es la dirección de ies?</td>\n",
       "      <td>(charla, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cuál es la dirección de ies?</td>\n",
       "      <td>(charla, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cuál es su dirección?</td>\n",
       "      <td>(charla, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dónde queda el ies?</td>\n",
       "      <td>(charla, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>quiero estudiar algo con rapida salida laboral</td>\n",
       "      <td>(trabajo, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>quiero estudiar algo para ganar plata</td>\n",
       "      <td>(trabajo, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>que requisitos tiene la cerrera</td>\n",
       "      <td>(generalidades, todas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>me faltan materias del secundario ¿me puedo an...</td>\n",
       "      <td>(tramites, requisitos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>tienen algun tipo de descuento?</td>\n",
       "      <td>(pagos, medios)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               oracion             intenciones\n",
       "0                                       qué es el ies?         (charla, todas)\n",
       "1                         cuál es la dirección de ies?         (charla, todas)\n",
       "2                         cuál es la dirección de ies?         (charla, todas)\n",
       "3                                cuál es su dirección?         (charla, todas)\n",
       "4                                  dónde queda el ies?         (charla, todas)\n",
       "..                                                 ...                     ...\n",
       "935     quiero estudiar algo con rapida salida laboral        (trabajo, todas)\n",
       "936              quiero estudiar algo para ganar plata        (trabajo, todas)\n",
       "937                    que requisitos tiene la cerrera  (generalidades, todas)\n",
       "938  me faltan materias del secundario ¿me puedo an...  (tramites, requisitos)\n",
       "939                    tienen algun tipo de descuento?         (pagos, medios)\n",
       "\n",
       "[940 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si paso dos listas a zip como entrada, el resultado será una tupla \n",
    "#donde cada elemento tendrá cada uno de los elementos pasados como entrada\n",
    "\n",
    "#df_textos['intenciones'] = df_textos[['intencion', 'subIntencion']].apply(tuple, axis=1)\n",
    "#df_textos[\"ff\"]=list(df_textos[['intencion', 'subIntencion']].itertuples(index=False, name=None))\n",
    "df_textos_tupla = df_textos.copy()\n",
    "df_textos_tupla[\"intenciones\"] = list(zip(df_textos_tupla.intencion, df_textos_tupla.subIntencion))\n",
    "df_textos_tupla.drop([\"intencion\",\"subIntencion\"],axis=1, inplace=True)\n",
    "df_textos_tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparativa = pd.DataFrame(columns=['Modelo','Umbral','Aciertos',\n",
    "'Errores','Indeterm','Precision','Recall','Accuracy','F1','df_Aciertos','df_Errores','df_Indeterm'])\n",
    "\n",
    "#Distancia_Coseno del profe\n",
    "\n",
    "def Evaluar_Modelo_Distancia_Coseno(X_train, X_test, y_test, umbral=0.8):\n",
    "    # Recorrer todo el Test Set prediciendo para cada oración de X_test y comparando con y_test\n",
    "    global df_comparativa\n",
    "    df_test = X_test[['oracion']].join(y_test)\n",
    "    array_X = X_train[X_train.columns[1:]].values\n",
    "    lista_distancia = [] #Distancia con el elemento más cercano (predicción)\n",
    "    lista_predic    = [] #Predicción del elemento más cercano\n",
    "    lista_similar   = [] #Texto del elemento más cercano\n",
    "    for test_doc in df_test.iterrows():\n",
    "        i,d = test_doc\n",
    "        df_query = pd.DataFrame([d['oracion']],columns=['oracion'])\n",
    "        Q = vectorizador.transform(df_query['oracion'])\n",
    "        distancia = [Distancia_Coseno(Q.A[0],fila) for fila in array_X]\n",
    "        df_Resultado = pd.DataFrame(distancia, columns=['Distancia']).join(X_train[['oracion']]).join(y_train[['intencion']]).sort_values(by='Distancia').head(1)\n",
    "\n",
    "        lista_distancia.append(df_Resultado.iloc[0]['Distancia'])\n",
    "        lista_predic.append(df_Resultado.iloc[0]['intencion']) \n",
    "        lista_similar.append(df_Resultado.iloc[0]['oracion'])\n",
    "\n",
    "    # Agregar columnas con resultados predichos al df_test\n",
    "    df_test['Distancia'] = lista_distancia\n",
    "    df_test['Predic']    = lista_predic\n",
    "    df_test['Similar']   = lista_similar\n",
    "\n",
    "    # Evaluar el resultado en df_test\n",
    "    print('Cantidad de registros evaluados:', len(df_test))\n",
    "    print('--------------------')\n",
    "    aciertos = df_test[ ( df_test['intencion'] == df_test['Predic'] ) & \n",
    "                        ( df_test['Distancia'] <= umbral ) ]['intencion'].count()\n",
    "    errores  = df_test[ ( df_test['intencion'] != df_test['Predic'] ) & \n",
    "                        ( df_test['Distancia'] <= umbral) ]['intencion'].count()\n",
    "    indeterm = df_test[ ( df_test['Distancia'] > umbral) ]['Distancia'].count()\n",
    "    print('Aciertos:', aciertos)\n",
    "    print('Errores :', errores)\n",
    "    print('Indeterm:', indeterm)\n",
    "    print('--------------------')\n",
    "    precision = aciertos/(aciertos+errores)\n",
    "    recall    = aciertos/(aciertos+indeterm)\n",
    "    accuracy  = (aciertos+indeterm)/(aciertos+errores+indeterm)\n",
    "    F1        = 2*((precision*recall)/(precision+recall))\n",
    "    print('Precision: {0:.3f} <- aciertos/(aciertos+errores)'.format(precision))\n",
    "    print('Recall   : {0:.3f} <- aciertos/(aciertos+indeterm)'.format(recall))\n",
    "    print('Accuracy : {0:.3f} <- (aciertos+indeterm)/(aciertos+errores+indeterm)'.format(accuracy))\n",
    "    print('F1       : {0:.3f} <- 2*((precision*recall)/(precision+recall))'.format(F1))\n",
    "\n",
    "    df_comparativa = df_comparativa.append({'Modelo': 'Distancia coseno',\n",
    "                                            'Umbral': umbral,\n",
    "                                            'Aciertos': aciertos,\n",
    "                                            'Errores': errores,\n",
    "                                            'Indeterm': indeterm,\n",
    "                                            'Precision': precision,\n",
    "                                            'Recall': recall,\n",
    "                                            'Accuracy': accuracy,\n",
    "                                            'F1': F1,\n",
    "                                            'df_Aciertos': df_test[ ( df_test['intencion'] == df_test['Predic'] ) & ( df_test['Distancia'] <= umbral ) ],\n",
    "                                            'df_Errores' : df_test[ ( df_test['intencion'] != df_test['Predic'] ) & ( df_test['Distancia'] <= umbral ) ],\n",
    "                                            'df_Indeterm': df_test[ ( df_test['Distancia'] > umbral ) ]\n",
    "                                            }, ignore_index=True)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros evaluados: 157\n",
      "--------------------\n",
      "Aciertos: 100\n",
      "Errores : 36\n",
      "Indeterm: 21\n",
      "--------------------\n",
      "Precision: 0.735 <- aciertos/(aciertos+errores)\n",
      "Recall   : 0.826 <- aciertos/(aciertos+indeterm)\n",
      "Accuracy : 0.771 <- (aciertos+indeterm)/(aciertos+errores+indeterm)\n",
      "F1       : 0.778 <- 2*((precision*recall)/(precision+recall))\n"
     ]
    }
   ],
   "source": [
    "Evaluar_Modelo_Distancia_Coseno(X_train, X_test, y_test, umbral=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.6757\n",
      "Mejor C: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Crear el modelo con los parámetros que no cambiarán: observar que no le pasamos el valor de C de regulrización\n",
    "RLog=LogisticRegression(penalty='none', max_iter=10000, tol=0.0001, multi_class='ovr',)\n",
    "\n",
    "# Armar el diccionario con el nombre y valores para los Hiperparámetros\n",
    "parametros_RLog = {'C':[1]}\n",
    "\n",
    "# Armar el GridSearchCV\n",
    "grid_RLog = GridSearchCV(estimator = RLog,scoring = 'accuracy',param_grid = parametros_RLog, cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "# Entrenar con el Train Set\n",
    "grid_RLog.fit(X_train[X_train.columns[1:]].values, y_train);\n",
    "\n",
    "# Obtener el mejor AC \n",
    "AC_RLog_best=grid_RLog.best_score_\n",
    "print('Mejor accuracy: ' + str(round(AC_RLog_best,4)))\n",
    "\n",
    "C_RLog_best=grid_RLog.best_params_ \n",
    "print('Mejor C: ' + str(C_RLog_best))\n",
    "###########################################################################################################\n",
    "def Evaluar_Modelo(modelo, nombre_modelo, X_test, y_test, umbral=0.7):\n",
    "    # Recorrer todo el Test Set prediciendo para cada oración de X_test y comparando con y_test\n",
    "    global df_comparativa\n",
    "    df_test = X_test[['oracion']].join(y_test)\n",
    "    lista_predic       = [] #Predicción\n",
    "    lista_probabilidad = [] #Probabilidad de la predicción\n",
    "    for test_doc in df_test.iterrows():\n",
    "        i,d = test_doc\n",
    "        df_query = pd.DataFrame([d['oracion']],columns=['oracion'])\n",
    "        Q = vectorizador.transform(df_query['oracion'])\n",
    "        pronostico = modelo.predict([Q.A[0]])\n",
    "        probabilidad = modelo.predict_proba([Q.A[0]])\n",
    "        lista_predic.append(pronostico[0])\n",
    "        lista_probabilidad.append(probabilidad[0].max())\n",
    "    \n",
    "    # Agregar columnas con resultados predichos al df_test\n",
    "    df_test['Probabilidad'] = lista_probabilidad\n",
    "    df_test['Predic'] = lista_predic\n",
    "    \n",
    "    # Evaluar el resultado en df_test\n",
    "    print('Cantidad de registros evaluados:', len(df_test))\n",
    "    print('--------------------')\n",
    "    aciertos = df_test[ ( df_test['intencion'] == df_test['Predic'] ) & \n",
    "                        ( df_test['Probabilidad'] >= umbral ) ]['intencion'].count()\n",
    "    errores  = df_test[ ( df_test['intencion'] != df_test['Predic'] ) & \n",
    "                        ( df_test['Probabilidad'] >= umbral) ]['intencion'].count()\n",
    "    indeterm = df_test[ ( df_test['Probabilidad'] < umbral) ]['Probabilidad'].count()\n",
    "    \n",
    "    print('Aciertos:', aciertos)\n",
    "    print('Errores :', errores)\n",
    "    print('Indeterm:', indeterm)\n",
    "    print('--------------------')\n",
    "    precision = aciertos/(aciertos+errores)\n",
    "    recall    = aciertos/(aciertos+indeterm)\n",
    "    accuracy  = (aciertos+indeterm)/(aciertos+errores+indeterm)\n",
    "    F1        = 2*((precision*recall)/(precision+recall))\n",
    "    print('Precision: {0:.3f} <- aciertos/(aciertos+errores)'.format(precision))\n",
    "    print('Recall   : {0:.3f} <- aciertos/(aciertos+indeterm)'.format(recall))\n",
    "    print('Accuracy : {0:.3f} <- (aciertos+indeterm)/(aciertos+errores+indeterm)'.format(accuracy))\n",
    "    print('F1       : {0:.3f} <- 2*((precision*recall)/(precision+recall))'.format(F1))\n",
    "\n",
    "    # Registrar Resultados\n",
    "    df_comparativa = df_comparativa.append({'Modelo': nombre_modelo,\n",
    "                                            'Umbral': umbral,\n",
    "                                            'Aciertos': aciertos,\n",
    "                                            'Errores': errores,\n",
    "                                            'Indeterm': indeterm,\n",
    "                                            'Precision': precision,\n",
    "                                            'Recall': recall,\n",
    "                                            'Accuracy': accuracy,\n",
    "                                            'F1': F1,\n",
    "                                            'df_Aciertos': df_test[ ( df_test['intencion'] == df_test['Predic'] ) & ( df_test['Probabilidad'] >= umbral ) ],\n",
    "                                            'df_Errores' : df_test[ ( df_test['intencion'] != df_test['Predic'] ) & ( df_test['Probabilidad'] >= umbral ) ],\n",
    "                                            'df_Indeterm': df_test[ ( df_test['Probabilidad'] < umbral ) ]\n",
    "                                            }, ignore_index=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros evaluados: 157\n",
      "--------------------\n",
      "Aciertos: 109\n",
      "Errores : 35\n",
      "Indeterm: 13\n",
      "--------------------\n",
      "Precision: 0.757 <- aciertos/(aciertos+errores)\n",
      "Recall   : 0.893 <- aciertos/(aciertos+indeterm)\n",
      "Accuracy : 0.777 <- (aciertos+indeterm)/(aciertos+errores+indeterm)\n",
      "F1       : 0.820 <- 2*((precision*recall)/(precision+recall))\n"
     ]
    }
   ],
   "source": [
    "Evaluar_Modelo(grid_RLog, 'Regresión Logística Sin Regularización', X_test, y_test, umbral=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Umbral</th>\n",
       "      <th>Aciertos</th>\n",
       "      <th>Errores</th>\n",
       "      <th>Indeterm</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>df_Aciertos</th>\n",
       "      <th>df_Errores</th>\n",
       "      <th>df_Indeterm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distancia coseno</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.826446</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.778210</td>\n",
       "      <td>oracion      inte...</td>\n",
       "      <td>oracion...</td>\n",
       "      <td>oracion     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regresión Logística Sin Regularización</td>\n",
       "      <td>0.8</td>\n",
       "      <td>109</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>0.756944</td>\n",
       "      <td>0.893443</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.819549</td>\n",
       "      <td>oracion      intencion...</td>\n",
       "      <td>oracion...</td>\n",
       "      <td>oracion      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Modelo  Umbral Aciertos Errores Indeterm  \\\n",
       "0                        Distancia coseno     0.8      100      36       21   \n",
       "1  Regresión Logística Sin Regularización     0.8      109      35       13   \n",
       "\n",
       "   Precision    Recall  Accuracy        F1  \\\n",
       "0   0.735294  0.826446  0.770701  0.778210   \n",
       "1   0.756944  0.893443  0.777070  0.819549   \n",
       "\n",
       "                                         df_Aciertos  \\\n",
       "0                               oracion      inte...   \n",
       "1                          oracion      intencion...   \n",
       "\n",
       "                                          df_Errores  \\\n",
       "0                                         oracion...   \n",
       "1                                         oracion...   \n",
       "\n",
       "                                         df_Indeterm  \n",
       "0                                    oracion     ...  \n",
       "1                                   oracion      ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_comparativa.iloc[0].df_Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_comparativa.iloc[1].df_Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.5976\n",
      "Mejor C: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "RLog=LogisticRegression(penalty='none', max_iter=10000, tol=0.0001, multi_class='ovr',)\n",
    "\n",
    "parametros_RLog = {'C':[1]}\n",
    "\n",
    "grid_RLog1 = GridSearchCV(estimator = RLog,scoring = 'accuracy',param_grid = parametros_RLog, cv = 5, n_jobs = -1)\n",
    "\n",
    "grid_RLog1.fit(X[X.columns[1:]].values, y)\n",
    "\n",
    "\n",
    "AC_RLog_best=grid_RLog1.best_score_\n",
    "print('Mejor accuracy: ' + str(round(AC_RLog_best,4)))\n",
    "\n",
    "C_RLog_best=grid_RLog1.best_params_ \n",
    "print('Mejor C: ' + str(C_RLog_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros evaluados: 157\n",
      "--------------------\n",
      "Aciertos: 154\n",
      "Errores : 0\n",
      "Indeterm: 3\n",
      "--------------------\n",
      "Precision: 1.000 <- aciertos/(aciertos+errores)\n",
      "Recall   : 0.981 <- aciertos/(aciertos+indeterm)\n",
      "Accuracy : 1.000 <- (aciertos+indeterm)/(aciertos+errores+indeterm)\n",
      "F1       : 0.990 <- 2*((precision*recall)/(precision+recall))\n"
     ]
    }
   ],
   "source": [
    "Evaluar_Modelo(grid_RLog1, 'Regresión Logística Sin Regularización', X_test, y_test, umbral=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Grabar el modelo elegido\n",
    "nombre_archivo='modelo_intents.sav'\n",
    "pickle.dump(grid_RLog1, open(nombre_archivo, 'wb'))\n",
    "\n",
    "# Grabar el vectorizador\n",
    "nombre_archivo='vectorizador_intents.sav'\n",
    "pickle.dump(vectorizador, open(nombre_archivo, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Grabar el modelo elegido\n",
    "\n",
    "pickle.dump(grid_RLog1, open(\"modelo_intents.sav\", \"wb\"))\n",
    "\n",
    "# Grabar el vectorizador\n",
    "pickle.dump(vectorizador, open(\"vectorizador_intents.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(df_textos, open(r'df_matriz_intents.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d333c3e79956f6cfdda154d497169890c9e1b3b648807dd58683480f0849f8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
